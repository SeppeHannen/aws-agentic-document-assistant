{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a557f2-928c-404a-80bb-c6d9970d037f",
   "metadata": {},
   "source": [
    "# 04 - Question answering using Retrieval Augmented Generation\n",
    "In this notebook we will be using the output of the `document_processing.ipynb` notebook to answer questions using retrieval augmented generation.\n",
    "\n",
    "Notebook overview:\n",
    "1. Load the PDFs as plaintext (output of `document_processing.ipynb`).\n",
    "2. Chunk the plaintext into a reasonable size, such that the LLMs context window can fit multiple chunks.\n",
    "3. Embed each of the chunks using an LLM and store the embeddings in a vector store.\n",
    "4. Answer a user question using the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14edbcf1-2b17-40c1-a983-38603c3f3414",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fb6206-6c73-4d33-8034-55f0480b37a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import botocore\n",
    "import boto3\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e72e75-5b72-4565-872f-5df636c61c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3.__version__, botocore.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab31147-83b2-4660-ad31-ef484ebd2581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssm_client = boto3.client(\"ssm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ef2b3-d7b3-4ea8-833e-29553dfb9b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_region_parameter = \"/AgenticLLMAssistant/bedrock_region\"\n",
    "bedrock_endpoint_parameter = \"/AgenticLLMAssistant/bedrock_endpoint\"\n",
    "s3_bucket_name_parameter = \"/AgenticLLMAssistant/AgentDataBucketParameter\"\n",
    "\n",
    "BEDROCK_REGION = ssm_client.get_parameter(Name=bedrock_region_parameter)\n",
    "BEDROCK_REGION = BEDROCK_REGION[\"Parameter\"][\"Value\"]\n",
    "\n",
    "S3_BUCKET_NAME = ssm_client.get_parameter(Name=s3_bucket_name_parameter)\n",
    "S3_BUCKET_NAME = S3_BUCKET_NAME[\"Parameter\"][\"Value\"]\n",
    "\n",
    "BEDROCK_REGION, S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a50c0-7772-4788-b0d2-2832afce55a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LLM_MODEL_ID = \"anthropic.claude-v1\"\n",
    "LLM_MODEL_ID = \"anthropic.claude-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e761fe-d3c9-4b77-ad10-73545ee5a00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retry_config = Config(\n",
    "    region_name=BEDROCK_REGION, retries={\"max_attempts\": 10, \"mode\": \"standard\"}\n",
    ")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee5ee2-d095-420f-908a-6575cb47f832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock\", config=retry_config)\n",
    "bedrock_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd91be5-9ef1-4c8e-8a17-1e402bc65977",
   "metadata": {},
   "source": [
    "### Load the PDFs as plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68b1cb-0265-4fc9-bfcd-70ee9e85021b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.helpers import load_list_from_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e29ad-7976-44ed-aca4-177e89cad9cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents_processed = load_list_from_s3(S3_BUCKET_NAME, \"documents_processed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eab01d-6f87-4a14-aec4-604c1bd2921c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(documents_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf059903-ad05-4abf-af67-378094c4de5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "We will be using Langchains vectorstores to store the embeddings, therefore, we will convert the documents to Langchain Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a0301-b374-462c-bf72-18aeeaf828b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "langchain_documents_text = []\n",
    "langchain_documents_tables = []\n",
    "\n",
    "for document in documents_processed:\n",
    "    document_name = document[\"name\"]\n",
    "    document_source_location = document[\"source_location\"]\n",
    "    document_s3_metadata = document[\"metadata\"]\n",
    "    print(document_s3_metadata)\n",
    "    mapping_to_original_page_numbers = {\n",
    "        idx: pg_num\n",
    "        for idx, pg_num in enumerate(json.loads(document_s3_metadata[\"pages_kept\"]))\n",
    "    }\n",
    "    # remove pages_kept since we already put the original page number.\n",
    "    del document_s3_metadata[\"pages_kept\"]\n",
    "\n",
    "    for page in document[\"pages\"]:\n",
    "        # Turn each page into a Langchain Document.\n",
    "        # Note: you could choose to also prepend part of the previous document\n",
    "        # and append part of the next document to include more context for documents\n",
    "        # that have many pages which continue their text on the next page.\n",
    "        current_metadata = {\n",
    "            \"document_name\": document_name,\n",
    "            \"document_source_location\": document_source_location,\n",
    "            \"page_number\": page[\"page\"],\n",
    "            \"original_page_number\": mapping_to_original_page_numbers[page[\"page\"]],\n",
    "        }\n",
    "        # merge the document_s3_metadata into the langchain Document metadata\n",
    "        # to be able to use them for filtering.\n",
    "        current_metadata.update(document_s3_metadata)\n",
    "\n",
    "        langchain_documents_text.append(\n",
    "            Document(page_content=page[\"page_text\"], metadata=current_metadata)\n",
    "        )\n",
    "\n",
    "        # Turn all the tables of the pages into seperate Langchain Documents as well\n",
    "        for table_markdown in page[\"page_tables\"]:\n",
    "            langchain_documents_tables.append(\n",
    "                Document(page_content=table_markdown, metadata=current_metadata)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8594299-e64a-4857-9ac9-4a27981a0f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Your documents contain {len(langchain_documents_text)}\"\n",
    "    f\" pages and {len(langchain_documents_tables)} tables combined\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6110d75c-93af-4524-ab36-d892ad5f06ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(langchain_documents_text), len(langchain_documents_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93116cc2-d831-4f02-bec4-294b6cb79435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_lengths = [len(doc.page_content) for doc in langchain_documents_text]\n",
    "\n",
    "print(\n",
    "    f\"Number of chunks: {len(chunk_lengths)},\"\n",
    "    f\" Max chunk length {max(chunk_lengths)},\"\n",
    "    f\" Min chunk length: {min(chunk_lengths)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d5e51-d36e-40cb-a7df-203827afbb56",
   "metadata": {
    "tags": []
   },
   "source": [
    "Plot the distribution of the length of the smaller chunks, to determine if some of them should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15dfc8d-7fbc-40d8-9208-a2cd26662030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_chunks_length_distribution(chunks, bin_size=50):\n",
    "    chunk_lengths = [len(doc.page_content) for doc in chunks]\n",
    "    bin_size = 50\n",
    "    num_bins = (max(chunk_lengths) - min(chunk_lengths)) // bin_size + 1\n",
    "\n",
    "    # Display smoothed histogram using seaborn\n",
    "    sns.histplot(chunk_lengths, kde=True, bins=num_bins, edgecolor=\"black\")\n",
    "    plt.xlabel(\"Chunk Length\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of Chunk Lengths\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4493f2fd-87af-4176-8564-ffec4d0b374b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_chunks_length_distribution(langchain_documents_text, bin_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402cd24-5993-44fa-b65d-ae3ca53ce4e8",
   "metadata": {},
   "source": [
    "### Chunk the Documents containing page text and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460eff03-e018-4692-86f5-9d64d484cc0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2454dc-8228-4aa1-b201-52df780e43a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# The chunk overlap duplicates some text across chunks to prevent context from being lost between chunks.\n",
    "# TODO: the following spliting uses tiktoken, create a custom one that use the tokenizer from anthropic.\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45eafc5-e907-4687-b000-2cf991aa43d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_documents_text_chunked = text_splitter.split_documents(\n",
    "    langchain_documents_text + langchain_documents_tables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f290d03-23ed-4be3-8ed1-f89af5a1a197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(langchain_documents_text_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ad0e9-708d-4e0d-b8b0-dc5862522011",
   "metadata": {},
   "source": [
    "Due to splitting of the chunks, we may have created some small chunks that are already captured in the chunk overlap.<br>\n",
    "It is recommended to have a look and estimate until which chunk length, the chunks can be removed.<br>\n",
    "The reason for this is that semantic search can occasionally favor chunks containing very little content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c5edf-ed17-4ff9-909a-7689937c5568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_chunks_length_distribution(langchain_documents_text_chunked, bin_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01045e-abff-4bfe-9603-93e31f77ce65",
   "metadata": {},
   "source": [
    "### Embed the documents and store them in a vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242869f7-1628-45cd-9909-1c3777210b75",
   "metadata": {},
   "source": [
    "For the embedding model we use Amazon Titan embedding model available through Amazon Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30873eab-a237-4694-b767-13df5cb5123d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock as LangchainBedrock\n",
    "\n",
    "# Define an embedding model to generate embeddings\n",
    "# TODO: request access for other embedding models\n",
    "embedding_model_id = \"amazon.titan-embed-text-v1\"\n",
    "embedding_model = BedrockEmbeddings(model_id=embedding_model_id, client=bedrock_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8aebe-703b-4692-a3ca-f3608c44fe9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Warning: The following lines of code create embedding for each document chunk and store them, this will use your embedding model for each document and incur costs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2d594-b1b4-4ae2-bab1-2ee027178701",
   "metadata": {},
   "source": [
    "TODO - implement the following improvements to the embedding generation process:\n",
    "\n",
    "* Decorrelate embedding generation from `FAISS.from_documents`.\n",
    "* Create the embedding independently and save them on S3, then update the code to attempt to load them from S3 before calling the embedding model.\n",
    "* Create the index from the embeddings and text pair, without calling the embedding API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dec29a-e893-407b-bf9b-eaf1d8950b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(langchain_documents_text_chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see a \"An error occurred (ResourceNotFoundException) when calling the InvokeModel operation: Could not resolve the foundation model from the provided model identifier.\" error when trying to run the following cell. If this happens then you should visit the \"Bedrock\" service in your AWS account and request for access to the models in the \"Model access\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840e7bb-28f6-4134-a3ef-9c716017c9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "faiss = FAISS.from_documents(langchain_documents_text_chunked, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bf2b0-4982-4f43-8535-3b421aa65693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faiss.save_local(\"faiss_vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b3d58-b8ed-40d2-a8fc-954beb408741",
   "metadata": {},
   "source": [
    "### Answer questions using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125089c-7867-45c6-a75c-247a5a27022d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Define a custom template for structured data extraction\n",
    "template = \"\"\"\\n\\nHuman:\n",
    "The following is a friendly conversation between a human and an AI assistant.\n",
    "The assistant is helpful and answers questions about documents based on extracts of those documents available within the <context></context> XML tags.\n",
    "When answering, the assistant abides by the rules defined in the <rules></rules> XML tags.\n",
    "\n",
    "The rules are below:\n",
    "<rules>\n",
    "The assistant answers the human questions accurately and concisely.\n",
    "When answering, the assistant relies on the context available in the <context></context> XML tags below.\n",
    "The assistant considers the conversation history available in the <history></history> XML tags when answering.\n",
    "If the assistant does not know the answer to a question, it truthfully says it does not know.\n",
    "The assistant answers in Markdown and puts the answer inside <response></response> XML tags.\n",
    "</rules>\n",
    "\n",
    "The retrieved document chuncks that the AI assistant uses to respond are below within the <context></context> XML tags, each document chunk is within a <document></document> XML tags:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Current conversation:\n",
    "<history>\n",
    "{history}\n",
    "</history>\n",
    "\n",
    "The user input to respond to is within the input XML tags below:\n",
    "<input>\n",
    "{input}\n",
    "</input>\n",
    "\\n\\nAssistant: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\", \"context\"], template=template\n",
    ")\n",
    "\n",
    "claude_llm = LangchainBedrock(\n",
    "    model_id=LLM_MODEL_ID,\n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={\"max_tokens_to_sample\": 512, \"temperature\": 0.0},\n",
    ")\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=claude_llm,\n",
    "    memory=ConversationBufferMemory(input_key=\"input\"),\n",
    "    prompt=PROMPT,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e5c30-8adc-4853-99fb-0f2607dc6d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_documents(retrieved_documents):\n",
    "    combined_documents = \"\"\n",
    "    # Add each of the extracts to the query\n",
    "    for idx, extract in enumerate(retrieved_documents):\n",
    "        combined_documents += f\"<document>{extract.page_content}</document>\\n\"\n",
    "\n",
    "    return combined_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4d8a0-510f-4768-8816-427943587c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"Who were in the board of directors of Amazon in 2021 and what were their positions?\"\n",
    "\n",
    "k = 4\n",
    "retrieved_chuncks = faiss.similarity_search(query=query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb89fd8-d6b5-458f-8148-8a4d8bbb4e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "response = conversation.predict(\n",
    "    input=query, context=combine_documents(retrieved_chuncks)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14de68-5bda-40ee-854f-c324dab6ab6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = response.strip().strip(\"<response>\").strip(\"</response>\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc849ee7-54fa-498d-85b8-e7549eb9459c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f66e8-4d1c-4785-80d4-3c6824873a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload semantic search index to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f213cc-b612-47ef-a07f-2b1e5863ef23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 sync ./faiss_vector_store s3://{S3_BUCKET_NAME}/semantic_search_index/faiss_vector_store/"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
